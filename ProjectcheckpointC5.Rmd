---
title: "Untitled"
author: "Joubin Zahrir"
date: "2025-06-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(catboost)
library(ggplot2)

# Load dataset
df <- read.csv("survey_us_salary_filtered.csv", stringsAsFactors = FALSE)

# Define categorical columns
cat_cols <- c("main_branch_label", "age_label", "remote_work_label",
              "ed_level_label", "dev_type_label", "org_size_label",
              "purchase_influence_label")

# Convert character columns to factors (do NOT modify NAs)
df[cat_cols] <- lapply(df[cat_cols], factor)

```

```{r}
# Count missing values in each column
na_counts <- sapply(df, function(x) sum(is.na(x)))
na_counts[na_counts > 0]  # Show only columns with missing values

```
```{r}
library(dplyr)

df_cleaned_subset <- df_cleaned %>%
  filter(!is.na(years_code_pro),
         !is.na(dev_type_label),
         !is.na(org_size_label))
```

```{r}
# Total number of rows (observations)
nrow(df_cleaned)

# Optional: Only show columns that still have any NAs
na_summary[na_summary > 0]
```
```{r}
library(catboost)
library(ggplot2)

# Define the final dataset
df <- df_cleaned_subset  # This should already contain only complete rows for the 3 variables

# Define the features (only 3)
features <- c("years_code_pro", "dev_type_label", "org_size_label")

# Split into train and test
set.seed(123)
train_index <- sample(nrow(df), 0.8 * nrow(df))
train <- df[train_index, ]
test <- df[-train_index, ]

# CatBoost Pools (no need for cat_idx since columns are already factors)
train_pool <- catboost.load_pool(data = train[features],
                                 label = log(train$converted_comp_yearly))  # log target

test_pool <- catboost.load_pool(data = test[features],
                                label = log(test$converted_comp_yearly))   # log target

# Model parameters
params <- list(
  loss_function = "RMSE",
  iterations = 1000,
  depth = 6,
  learning_rate = 0.03,
  eval_metric = "R2",
  random_seed = 123,
  early_stopping_rounds = 20,
  verbose = 0
)

# Train the model
model <- catboost.train(train_pool, test_pool, params = params)

# Predict
pred_log <- catboost.predict(model, test_pool)
pred <- exp(pred_log)  # Convert log_salary back to salary

# Evaluate model
SSE <- sum((test$converted_comp_yearly - pred)^2)
SST <- sum((test$converted_comp_yearly - mean(test$converted_comp_yearly))^2)
R2 <- 1 - SSE/SST
RMSE <- sqrt(mean((test$converted_comp_yearly - pred)^2))
cat("R^2 =", round(R2, 3), "   RMSE =", round(RMSE, 0), "\n")

# Feature importance plot
importance <- catboost.get_feature_importance(model, pool = train_pool)
importance_df <- data.frame(Feature = features, Importance = importance)
importance_df <- importance_df[order(-importance_df$Importance), ]

ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_col(fill = "#4682B4") +
  coord_flip() +
  labs(title = "Feature Importance", x = "Feature", y = "Importance")

```

